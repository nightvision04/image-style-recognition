{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import connections as con\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "def json_loads(a):\n",
    "    return np.array(json.loads(a))\n",
    "\n",
    "class X_data:\n",
    "    ''' This object will contain the models active from the database\n",
    "        active_dfs = Active_dfs()\n",
    "        # Creates: Active_dfs().df_dict[pair] for each active pair\n",
    "    '''\n",
    "    def __init__(self,connection,table):\n",
    "        ''' This is run whenever the object is first created\n",
    "        '''\n",
    "\n",
    "        self.df =  con.get_id_strips(connection,table)\n",
    "        self.df['x'] = self.df.apply(lambda row: json_loads(row['x']), axis=1)\n",
    "        self.df['tags'] = self.df.apply(lambda row: json_loads(row['tags']), axis=1)\n",
    "        print('Loaded json models')\n",
    "\n",
    "\n",
    "def load_data(target_name,target_type):\n",
    "    connection = con.get_connection('image_profile')\n",
    "    control_table = 'flickr_' + target_type\n",
    "    control = X_data(connection,control_table)\n",
    "\n",
    "\n",
    "    control_x, X_test, control_y, y_test = train_test_split(\n",
    "        control.df.x.values, np.zeros(len(control.df.x.values)), test_size=0.05, random_state=42,shuffle=True)\n",
    "    connection.close()\n",
    "\n",
    "\n",
    "\n",
    "    table_name = target_name + '_' + target_type\n",
    "\n",
    "    connection = con.get_connection('image_profile')\n",
    "    target = X_data(connection,table_name)\n",
    "    connection.close()\n",
    "    target.df.loc[target.df['label']==target_name ,'y'] = 1\n",
    "\n",
    "    X_target = target.df.x.values\n",
    "    y_target = target.df.y.values\n",
    "    X_control = control_x\n",
    "    y_control = control_y\n",
    "\n",
    "    # Each time this function is run, the data conforms closer to the target contrast distribution, at the cost of samples.\n",
    "    for i in range(2):\n",
    "        X_target,y_target,X_control,y_control = fit_training_data(X_target,y_target,X_control,y_control)\n",
    "    \n",
    "    X = np.concatenate((X_control, X_target), axis=0)\n",
    "    y = np.concatenate((y_control, y_target), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    return X,y\n",
    "\n",
    "def test_split(X,y):\n",
    "    # This fixes the 'setting an array index with a sequence' ValueError\n",
    "\n",
    "    arr = np.zeros(len(X),dtype=object)\n",
    "    for i in range(len(X)):\n",
    "        arr[i]=X[i]\n",
    "\n",
    "    arr = np.array(arr.tolist())\n",
    "    X = arr.reshape(len(X),len(X[0]))\n",
    "\n",
    "    arr = np.zeros(len(y),dtype=object)\n",
    "    for i in range(len(y)):\n",
    "        arr[i]=y[i]\n",
    "\n",
    "    arr = np.array(arr.tolist())\n",
    "    y = arr.reshape(len(y),1)\n",
    "\n",
    "\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=42,shuffle=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def generate_model(X_train, X_test, y_train, y_test,target_name,target_type):\n",
    "\n",
    "    print('Training size:',len(X_train))\n",
    "    print('Testing size:',len(X_test))\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(0.98)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    pipeline = Pipeline([('scaler', sc), ('pca', pca),('rforest',rf)])\n",
    "\n",
    "    # Instantiate the grid search model\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "    # Create the parameter grid based on the results of random search\n",
    "    param_grid = {\n",
    "        'pca__n_components': [0.87,0.89,0.91,0.93,0.95],\n",
    "        'rforest__bootstrap': [True],\n",
    "        'rforest__max_depth': [100,110],\n",
    "        'rforest__max_features': [0.3],\n",
    "        'rforest__min_samples_leaf': [3],\n",
    "        'rforest__min_samples_split': [8],\n",
    "        'rforest__n_estimators': [1200,1400]\n",
    "    }\n",
    "\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid = param_grid,cv = 2, n_jobs = 6, verbose = 2)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "    # best_params__ = {'bootstrap': True,\n",
    "    #                  'max_depth': 90,\n",
    "    #                  'max_features': 2,\n",
    "    #                  'min_samples_leaf': 3,\n",
    "    #                  'min_samples_split': 8,\n",
    "    #                  'n_estimators': 300}\n",
    "\n",
    "    model = grid_search.best_estimator_\n",
    "    print('best params:',grid_search.best_params_)\n",
    "    y_pred = model.predict(X_test)\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    model_name = '../models/' +target_name + '_' + target_type + '.pickle'\n",
    "\n",
    "    from sklearn.externals import joblib\n",
    "    joblib.dump(model, model_name)\n",
    "\n",
    "    return True\n",
    "\n",
    "# def store_model(dict_,filename):\n",
    "#     with open(filename, 'wb') as handle:\n",
    "#         pickle.dump(dict_, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     return True\n",
    "\n",
    "def load_model(file):\n",
    "    # Load data (deserialize)\n",
    "    with open(file, 'rb') as handle:\n",
    "        unserialized_data = pickle.load(handle)\n",
    "    return unserialized_data\n",
    "\n",
    "def start_model(target_name,target_type):\n",
    "    X,y = load_data(target_name,target_type)\n",
    "    X_train, X_test, y_train, y_test = test_split(X,y)\n",
    "    generate_model(X_train, X_test, y_train, y_test,target_name,target_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_X_stats(X,X_control):\n",
    "\n",
    "    from scipy import stats\n",
    "\n",
    "    square_size = 30\n",
    "    X_morph = []\n",
    "    for i in range(len(X)):\n",
    "\n",
    "        hsl_features = np.zeros([1,3])    \n",
    "        img =  X[i].reshape(int(len(X[i]) / (square_size*3)),int(len(X[i]) / (square_size*3)),3 ).astype('uint8')\n",
    "        # Get stats for hue, contrast, saturation\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        # Convert to float64 for higher precision stat descriptions\n",
    "        hsv = hsv.reshape(int(len(X[i]) / 3),3 ).astype('float32')\n",
    "        val_std = np.std(hsv, axis=0)[2] # Value std_dev\n",
    "        X_morph.append(val_std)\n",
    "\n",
    "    X_morph = np.array(X_morph)\n",
    "\n",
    "    square_size = 30\n",
    "    X_morph_control = []\n",
    "    for i in range(len(X_control)):\n",
    "\n",
    "        hsl_features = np.zeros([1,3])    \n",
    "        img =  X_control[i].reshape(int(len(X_control[i]) / (square_size*3)),int(len(X_control[i]) / (square_size*3)),3 ).astype('uint8')\n",
    "        # Get stats for hue, contrast, saturation\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        # Convert to float64 for higher precision stat descriptions\n",
    "        hsv = hsv.reshape(int(len(X_control[i]) / 3),3 ).astype('float32')\n",
    "        val_std = np.std(hsv, axis=0)[2] # Value std_dev\n",
    "\n",
    "        X_morph_control.append(val_std)\n",
    "\n",
    "    X_morph_control = np.array(X_morph_control)\n",
    "    \n",
    "    return X_morph,X_morph_control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bin_sizes(sample_length):\n",
    "\n",
    "    from scipy.stats import truncnorm\n",
    "    def get_truncated_normal(mean=50, sd=20, low=0, upp=100):\n",
    "        return truncnorm(\n",
    "            (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "    norm_dist = get_truncated_normal(mean=50, sd=20, low=0, upp=100)\n",
    "    res = 1000000\n",
    "    norm_dist_shape = norm_dist.rvs(res)\n",
    "    \n",
    "    # Set bin sizes - eg (bin_sizes[0] means between 0-4, bin_sizes[5] means between 5-9, etc..)\n",
    "    bin_sizes ={}\n",
    "    bin_count = {}\n",
    "    for i in range(0,130,5):\n",
    "        bin_sizes[i] = int(len(norm_dist_shape[(norm_dist_shape >= i) & (norm_dist_shape < (i+5))]) / res * sample_length)\n",
    "        bin_count[i] = 0\n",
    "    return bin_sizes, bin_count\n",
    "\n",
    "\n",
    "def conform_x_to_dist(X,bin_sizes,bin_count):\n",
    "\n",
    "    X_bool = []\n",
    "    \n",
    "    for val in X:\n",
    "        yep=False\n",
    "        for i in range(0,130,5):\n",
    "            if (val >= i) & (val < (i+5)) :\n",
    "                if bin_count[i] < bin_sizes[i]:\n",
    "                    yep= True\n",
    "                    bin_count[i]+=1\n",
    "                    X_bool.append(True)\n",
    "                else:\n",
    "                    yep= True\n",
    "                    X_bool.append(False)\n",
    "        if yep==False:\n",
    "            print(val)\n",
    "            \n",
    "    return np.array(X_bool)\n",
    "\n",
    "def fit_training_data(X,y,X_control,y_control):\n",
    "\n",
    "    # Get hsv stats for training sets\n",
    "    X_morph,X_morph_control = generate_X_stats(X,X_control)\n",
    "\n",
    "    # Resize training sets to reflect normally distributed contrast complexity.\n",
    "    X_morph_bin_sizes,X_morph_bin_count = generate_bin_sizes(len(X_morph))\n",
    "    X_bool = conform_x_to_dist(X_morph,X_morph_bin_sizes,X_morph_bin_count)\n",
    "    X = X[X_bool==True]\n",
    "    y = y[X_bool==True]\n",
    "\n",
    "    X_morph_control_bin_sizes,X_morph_control_bin_count = generate_bin_sizes(len(X_morph))\n",
    "    X_bool_control = conform_x_to_dist(X_morph_control,X_morph_control_bin_sizes,X_morph_control_bin_count)\n",
    "    X_control = X_control[X_bool_control==True]\n",
    "    y_control = y_control[X_bool_control==True]\n",
    "    \n",
    "    return X,y,X_control,y_control\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.distplot(X_morph[X_bool])\n",
    "# sns.distplot(X_morph_control[X_bool_control])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayscale\n",
    "{'pca__n_components': 0.93,\n",
    " 'rforest__bootstrap': True,\n",
    " 'rforest__max_depth': 110,\n",
    " 'rforest__max_features': 0.3,\n",
    " 'rforest__min_samples_leaf': 3,\n",
    " 'rforest__min_samples_split': 8,\n",
    " 'rforest__n_estimators': 1200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color\n",
    "\n",
    "{'pca__n_components': 0.87,\n",
    " 'rforest__bootstrap': True,\n",
    " 'rforest__max_depth': 110,\n",
    " 'rforest__max_features': 0.3,\n",
    " 'rforest__min_samples_leaf': 3,\n",
    " 'rforest__min_samples_split': 8,\n",
    " 'rforest__n_estimators': 1200}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
