{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import connections as con\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def json_loads(a):\n",
    "    return np.array(json.loads(a))\n",
    "    \n",
    "class X_data:\n",
    "    ''' This object will contain the models active from the database\n",
    "        active_dfs = Active_dfs()\n",
    "        # Creates: Active_dfs().df_dict[pair] for each active pair\n",
    "    '''\n",
    "    def __init__(self,connection,table):\n",
    "        ''' This is run whenever the object is first created\n",
    "        '''\n",
    "        \n",
    "        self.df =  con.get_id_strips(connection,table)\n",
    "        self.df['x'] = self.df.apply(lambda row: json_loads(row['x']), axis=1)\n",
    "        self.df['tags'] = self.df.apply(lambda row: json_loads(row['tags']), axis=1)\n",
    "        print('Loaded json models')\n",
    "    \n",
    "    \n",
    "def load_data(target_name,target_type):\n",
    "    connection = con.get_connection('image_profile')\n",
    "    control = X_data(connection,'flickr_convolution')    \n",
    "    connection.close()\n",
    "    control.df.loc[control.df['label']=='flickr' ,'y'] = 0\n",
    "\n",
    "    table_name = target_name + '_' + target_type\n",
    "\n",
    "    connection = con.get_connection('image_profile')\n",
    "    target = X_data(connection,table_name)    \n",
    "    connection.close()\n",
    "    target.df.loc[target.df['label']==target_name ,'y'] = 1\n",
    "\n",
    "    # Remove sample bias\n",
    "    target_len = len(target.df.x.values)\n",
    "    control_len = len(control.df.x.values)\n",
    "    print('target length',target_len)\n",
    "    print('control length',control_len)\n",
    "   \n",
    "    if target_len > control_len:\n",
    "        max_len = control_len -1\n",
    "    else:\n",
    "        max_len = target_len -1\n",
    "    print('max length',max_len)\n",
    "    X = np.concatenate((control.df.x.values[0:max_len], target.df.x.values[0:max_len]), axis=0)\n",
    "    y = np.concatenate((control.df.y.values[0:max_len], target.df.y.values[0:max_len]), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return X,y\n",
    "\n",
    "def test_split(X,y):\n",
    "    # This fixes the 'setting an array index with a sequence' ValueError\n",
    "    arr = np.zeros(len(X),dtype=object)\n",
    "    for i in range(len(X)): \n",
    "        arr[i]=X[i]\n",
    "\n",
    "    arr = np.array(arr.tolist())\n",
    "    X = arr.reshape(len(X),len(X[0]))\n",
    "\n",
    "    arr = np.zeros(len(y),dtype=object)\n",
    "    for i in range(len(y)): \n",
    "        arr[i]=y[i]\n",
    "\n",
    "    arr = np.array(arr.tolist())\n",
    "    y = arr.reshape(len(y),1)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=42,shuffle=True)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def generate_model(X_train, X_test, y_train, y_test,target_name,target_type):\n",
    "\n",
    "    print('Training size:',len(X_train))\n",
    "    print('Testing size:',len(X_test))\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    sc = StandardScaler()  \n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(0.98)  \n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rf = RandomForestClassifier()\n",
    "\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    pipeline = Pipeline([('scaler', sc), ('pca', pca),('rforest',rf)])\n",
    "\n",
    "    # Instantiate the grid search model\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    \n",
    "    \n",
    "    # Create the parameter grid based on the results of random search \n",
    "    param_grid = {\n",
    "        'pca__n_components': [0.89],\n",
    "        'rforest__bootstrap': [True],\n",
    "        'rforest__max_depth': [110],\n",
    "        'rforest__max_features': [0.3],\n",
    "        'rforest__min_samples_leaf': [3],\n",
    "        'rforest__min_samples_split': [8],\n",
    "        'rforest__n_estimators': [1200]\n",
    "    }\n",
    "    \n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid = param_grid,cv = 2, n_jobs = 6, verbose = 2)\n",
    "\n",
    "    # Fit the grid search to the data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "\n",
    "    # best_params__ = {'bootstrap': True,\n",
    "    #                  'max_depth': 90,\n",
    "    #                  'max_features': 2,\n",
    "    #                  'min_samples_leaf': 3,\n",
    "    #                  'min_samples_split': 8,\n",
    "    #                  'n_estimators': 300}\n",
    "\n",
    "    model = grid_search.best_estimator_\n",
    "    print('best params:',grid_search.best_params_)\n",
    "    y_pred = model.predict(X_test)\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    model_name = '../models/' +target_name + '_' + target_type + '.pickle'\n",
    "    \n",
    "    from sklearn.externals import joblib\n",
    "    joblib.dump(model, model_name)\n",
    "    \n",
    "    return True\n",
    "\n",
    "# def store_model(dict_,filename):\n",
    "#     with open(filename, 'wb') as handle:\n",
    "#         pickle.dump(dict_, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     return True\n",
    "\n",
    "def load_model(file):\n",
    "    # Load data (deserialize)\n",
    "    with open(file, 'rb') as handle:\n",
    "        unserialized_data = pickle.load(handle)\n",
    "    return unserialized_data\n",
    "\n",
    "def start_model(target_name,target_type):\n",
    "    X,y = load_data(target_name,target_type)\n",
    "    X_train, X_test, y_train, y_test = test_split(X,y)\n",
    "    generate_model(X_train, X_test, y_train, y_test,target_name,target_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded json models\n",
      "Loaded json models\n",
      "target length 44400\n",
      "control length 44016\n",
      "max length 44015\n"
     ]
    }
   ],
   "source": [
    "#X,y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 58980\n",
      "Testing size: 29050\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   2 | elapsed: 14.2min finished\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\pipeline.py:250: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'pca__n_components': 0.89, 'rforest__bootstrap': True, 'rforest__max_depth': 110, 'rforest__max_features': 0.3, 'rforest__min_samples_leaf': 3, 'rforest__min_samples_split': 8, 'rforest__n_estimators': 1200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.70      0.70      0.70     14551\n",
      "        1.0       0.70      0.70      0.70     14499\n",
      "\n",
      "avg / total       0.70      0.70      0.70     29050\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "10x10\n",
    "best params: {\n",
    "'rforest__bootstrap': True,\n",
    "'rforest__max_depth': 110,\n",
    "'rforest__max_features': 2,\n",
    "'rforest__min_samples_leaf': 3,\n",
    "'rforest__min_samples_split': 8,\n",
    "'rforest__n_estimators': 300}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "25x25\n",
    "best params: {'pca__n_components': 0.89,\n",
    "              'rforest__bootstrap': True,\n",
    "              'rforest__max_depth': 110,\n",
    "              'rforest__max_features': 0.3,\n",
    "              'rforest__min_samples_leaf': 3,\n",
    "              'rforest__min_samples_split': 8,\n",
    "              'rforest__n_estimators': 1500}\n",
    "       \n",
    "       \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
